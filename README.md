# pca-pyspark
Optimizing the number of Principal Components for dimensionality reduction in PySpark.

This Python 3 notebook was compiled on the Databricks environment:
  + Databricks runtime version: 5.2 (includes Apache Spark 2.4.0, Scala 2.11).
  
In it we show how to use PCA and how to obtain the optimum number of PCs in PySpark.

To load the dataset it must be uploaded to the Databricks Data section.
